{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c35b5e2014f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "def get_count_vect_corpus(texts):\n",
    "    '''\n",
    "    Тут tf_vectorizer, потому что без\n",
    "    idf=true он такой же, как и count vectors\n",
    "    но с нормализацией\n",
    "    '''\n",
    "    return tf_vectorizer.fit_transform(texts)\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import json\n",
    "# import gensim\n",
    "# from gensim.models.wrappers import FastText\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from pymystem3 import Mystem\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\")\n",
    "# russian_stopwords = stopwords.words(\"russian\")\n",
    "# mystem = Mystem()\n",
    "# model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "# bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "# # Функция индексирования данных.\n",
    "# # На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "# def inverted_indexing(corpus, df, vectorizer):\n",
    "#     X = vectorizer.fit_transform(corpus)\n",
    "#     return X\n",
    "\n",
    "# # def similarity_array(query, df):\n",
    "# #     return cosine_similarity(query, df.values)\n",
    "# def get_cosine_similarity(sparse_matrix, query):\n",
    "#     return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "#     df = df.head(10000)\n",
    "#     corpus = df['ans_lemmas'].tolist()\n",
    "#     X = inverted_indexing(corpus, df, count_vectorizer)\n",
    "#     pickle.dump(count_vectorizer, open('../vectorizers/tfidf_vectorizer.pickle', 'wb'))\n",
    "#     scipy.sparse.save_npz('../corpora/tfidf_vectorizer.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w.isalpha()])\n",
    "    return re.sub('\\n', '', ans)\n",
    "\n",
    "\n",
    "def make_df_from_corpus(path_to_json:str):\n",
    "    with open(path_to_json, 'r') as f:\n",
    "        qa_corpus = list(f)[:10500]\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for qa in qa_corpus:\n",
    "      qa = json.loads(qa)\n",
    "      if qa['answers'] != []:\n",
    "        max_value = -10 ** 6\n",
    "        max_text = ''\n",
    "        for answer in qa['answers']:\n",
    "          if answer['author_rating']['value'] != '':\n",
    "            cur_value = int(answer['author_rating']['value'])\n",
    "            if cur_value >= max_value:\n",
    "              max_text = answer['text']\n",
    "        if max_text != '':\n",
    "          answers.append(max_text)\n",
    "          questions.append(qa['question'])\n",
    "    df = pd.DataFrame({'questions': questions, 'answers': answers})\n",
    "    df = df[df['answers'].notnull()]\n",
    "    df = df[df['questions'].notnull()]\n",
    "    df = df.reset_index()\n",
    "    df = df.head(10000)\n",
    "    df['ans_lemmas'] = df['answers'].apply(preprocess_line)\n",
    "    df['quest_lemmas'] = df['questions'].apply(preprocess_line)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_indexing(vectorizer, corpus, df, column_name):\n",
    "    if column_name == 'answers':\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "    else:\n",
    "        X = vectorizer.transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = TfidfVectorizer(use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "corpus_answers = df['ans_lemmas'].tolist()\n",
    "corpus_questions = df['quest_lemmas'].tolist()\n",
    "answers_matrix = inverted_indexing(count_vectorizer, corpus_answers, df, 'answers')\n",
    "questions_matrix = inverted_indexing(count_vectorizer, corpus_questions, df, 'questions')\n",
    "\n",
    "pickle.dump(count_vectorizer, open('../vectorizers/count_vectorizer.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('../corpora/count_vectorizer_answers.npz', answers_matrix)\n",
    "scipy.sparse.save_npz('../corpora/count_vectorizer_questions.npz', questions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9897                                            Не любит.\n",
       "4673                             Не люблю и не усложняю))\n",
       "7103                                не уважает и не любит\n",
       "2874                                  я не люблю поцелуи.\n",
       "5213                                    любить не стыдно)\n",
       "                              ...                        \n",
       "5172                         тебе надо навязчивую девушку\n",
       "5173      Наверно скорее всего над тобой хочет посмеяться\n",
       "5176                 Хз, действуй по животным инстинктам.\n",
       "5177    Разное))) ну у некоторых людей у большенства о...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "answers_matrix = sparse.load_npz('../corpora/count_vectorizer_answers.npz')\n",
    "\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "query = count_vectorizer.transform([query])\n",
    "\n",
    "sim_array = get_cosine_similarity(answers_matrix, query)\n",
    "sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "\n",
    "answers_matrix = inverted_indexing(tfidf_vectorizer, corpus_answers, df, 'answers')\n",
    "questions_matrix = inverted_indexing(tfidf_vectorizer, corpus_questions, df, 'questions')\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open('../vectorizers/tfidf_vectorizer.pickle', 'wb'))\n",
    "scipy.sparse.save_npz('../corpora/tfidf_vectorizer_answers.npz', answers_matrix)\n",
    "scipy.sparse.save_npz('../corpora/tfidf_vectorizer_questions.npz', questions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9897                                            Не любит.\n",
       "3413                                               Любят.\n",
       "8100                                      Не любящий всех\n",
       "3956                               Любит она тебя, любит!\n",
       "2426              Нет. Любить всех -это не любить никого.\n",
       "                              ...                        \n",
       "5048                                 Ииииипать, эксперты!\n",
       "5050                                Затащи его в постель.\n",
       "5051    Однако потом он может потребовать от тебя что ...\n",
       "5054                                         он ревнует))\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "answers_matrix = sparse.load_npz('../corpora/tfidf_vectorizer_answers.npz')\n",
    "\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "query = tfidf_vectorizer.transform([query])\n",
    "\n",
    "sim_array = get_cosine_similarity(answers_matrix, query)\n",
    "sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('qa.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer, column_name):\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
    "    if column_name == 'answers':\n",
    "        tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
    "        cv_matrix = count_vectorizer.fit_transform(corpus)\n",
    "        idf = tfidf_vectorizer.idf_\n",
    "        tf = tf_vectorizer\n",
    "        pickle.dump(idf, open('../vectorizers/idf_vectorizer.pickle', 'wb'))\n",
    "        pickle.dump(tf, open('../vectorizers/tf_vectorizer.pickle', 'wb'))\n",
    "        pickle.dump(count_vectorizer, open('../vectorizers/cv_vectorizer.pickle', 'wb'))\n",
    "    else:\n",
    "        tf_matrix = tf_vectorizer.transform(corpus)\n",
    "        cv_matrix = count_vectorizer.transform(corpus)\n",
    "    idf = tfidf_vectorizer.idf_\n",
    "    idf = np.expand_dims(idf, axis=0)\n",
    "    idf = sparse.csr_matrix(idf)\n",
    "    return tf_matrix, idf, cv_matrix\n",
    "\n",
    "# def BM_25(tf_matrix, idf_matrix, cv_matrix, k=2, b=0.75):\n",
    "#     values = []\n",
    "#     rows = []\n",
    "#     cols = []\n",
    "#     len_d = cv_matrix.sum(axis=1).T\n",
    "#     avgdl = len_d.mean()\n",
    "#     for i, j in zip(*tf_matrix.nonzero()):\n",
    "#         A = idf_matrix[0,j] * tf_matrix[i, j] * (k+1)\n",
    "#         B_1 = (k * (1 - b + b * len_d[0,i] / avgdl))\n",
    "#         B_1 = np.expand_dims(B_1, axis=-1) \n",
    "#         B = tf_matrix[i, j] + B_1\n",
    "#         B = B[0]\n",
    "#         values.append(A/B)\n",
    "#         rows.append(i)\n",
    "#         cols.append(j)\n",
    "#     sparse_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "# #     pickle.dump(sparse_matrix, open('./BM25.pickle', 'wb'))\n",
    "#     return sparse_matrix\n",
    "def BM_25(tf_matrix, idf_matrix, cv_matrix, k=2, b=0.75):\n",
    "    len_d = cv_matrix.sum(axis=1)\n",
    "    avgdl = len_d.mean()\n",
    "    num = tf_matrix.multiply(idf_matrix) * (k + 1)\n",
    "    mask = sparse.csr_matrix(tf_matrix.shape)\n",
    "    mask[tf_matrix.nonzero()] = 1\n",
    "    div =(tf_matrix + mask.multiply(k * (1 - b + b * len_d / avgdl)))\n",
    "    div[div.nonzero()] = 1 / div[div.nonzero()]\n",
    "    return num.multiply(div)\n",
    "# len_d = x_count_vec.sum(axis=1)\n",
    "#     avdl = len_d.mean()\n",
    "#     B_1 = (k * (1 - b + b * len_d / avdl))\n",
    "#     B_1 = np.expand_dims(B_1, axis=-1)\n",
    "\n",
    "#     for i, j in zip(*tf.nonzero()):\n",
    "#         rows.append(i)\n",
    "#         cols.append(j)\n",
    "#         A = tf[i, j] * idf[0][j] * (k + 1)\n",
    "#         B = tf[i, j] + B_1[i]\n",
    "#         value = A / B\n",
    "#         values.append(value[0][0])\n",
    "\n",
    "#     sparse_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "# count_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "#     tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "\n",
    "tf_matrix1, idf_matrix1, cv_matrix1 = doc_term_matrix(corpus_answers, tfidf_vectorizer, tf_vectorizer, 'answers')\n",
    "\n",
    "answers_matrix = BM_25(tf_matrix1, idf_matrix1, cv_matrix1)\n",
    "scipy.sparse.save_npz('../corpora/BM25_answers.npz', answers_matrix)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "tf_vectorizer = pickle.load(open('../vectorizers/tf_vectorizer.pickle', 'rb'))\n",
    "\n",
    "tf_matrix2, idf_matrix2, cv_matrix2 = doc_term_matrix(corpus_questions, tfidf_vectorizer, tf_vectorizer, 'questions')\n",
    "questions_matrix = BM_25(tf_matrix2, idf_matrix2, cv_matrix2)\n",
    "scipy.sparse.save_npz('../corpora/BM25_questions.npz', questions_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anwers_matrix = sparse.load_npz('../corpora/BM25_answers.npz')\n",
    "questions_matrix = sparse.load_npz('../corpora/BM25_questions.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('../corpora/BM25_answers.npz', answers_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14900)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.load_npz('../corpora/BM25_answers.npz').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14900)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.load_npz('../corpora/BM25_questions.npz').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14900) (10000, 14900)\n"
     ]
    }
   ],
   "source": [
    "# scipy.sparse.save_npz('../corpora/BM25_answers.npz', answers_matrix)\n",
    "\n",
    "anwers_matrix = sparse.load_npz('../corpora/BM25_answers.npz')\n",
    "questions_matrix = sparse.load_npz('../corpora/BM25_questions.npz')\n",
    "print(anwers_matrix.shape, questions_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../corpora/qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9897                                            Не любит.\n",
       "8100                                      Не любящий всех\n",
       "5213                                    любить не стыдно)\n",
       "2874                                  я не люблю поцелуи.\n",
       "3413                                               Любят.\n",
       "                              ...                        \n",
       "4587                        Долго ждали, что б спросить))\n",
       "4589                                   Страх одиночества.\n",
       "4590                     Дважды в одну реку нельзя войти.\n",
       "4592    Подождать, пока поправится. Простуда действите...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "answers_matrix = sparse.load_npz('../corpora/BM25_answers.npz')\n",
    "\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "query = count_vectorizer.transform([query])\n",
    "\n",
    "sim_array = get_cosine_similarity(answers_matrix, query)\n",
    "sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_normalization(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = vec_normalization(np.array(emb_list))\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "# def no_empties(line):\n",
    "#     if not isinstance(line,str):\n",
    "#         return 'нет слов'\n",
    "#     elif len(line) < 1:\n",
    "#         return 'нет слов'\n",
    "#     elif line == '':\n",
    "#         return 'нет слов'\n",
    "\n",
    "def save_embed_corpus(df, column_name):\n",
    "    df['ans_embeds'] = df[column_name].apply(make_ft_embedding)\n",
    "    split_df = pd.DataFrame(df['ans_embeds'].tolist(),\n",
    "                        columns=[f'word{i}' for i in range(300)])\n",
    "#     split_df['doc_name'] = df[column_name]\n",
    "    split_df.to_csv(f'../corpora/corpus_ft_{column_name}.csv')\n",
    "    return split_df\n",
    "\n",
    "    \n",
    "def save_bert_corpus(texts, model, tokenizer, column_name):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "      t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "      with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "      embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "      embeddings = torch.nn.functional.normalize(embeddings)\n",
    "      vectors.append(embeddings[0].cpu().numpy())\n",
    "    BERT_corpus = sparse.csr_matrix(vectors)\n",
    "    sparse.save_npz(f'../corpora/BERT{column_name}.npz', BERT_corpus)\n",
    "    return sparse.csr_matrix(vectors)\n",
    "\n",
    "\n",
    "# def get_query_bert(query):\n",
    "#     cls_embeddings = get_bert_corpus(query, b_model, b_tokenizer)\n",
    "#     return sparse.csr_matrix(cls_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>ans_lemmas</th>\n",
       "      <th>quest_lemmas</th>\n",
       "      <th>ans_embeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>5070</td>\n",
       "      <td></td>\n",
       "      <td>Никак. Любит-не любит — это вымышленные катего...</td>\n",
       "      <td>никак любить не любить это вымышленный категор...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index questions                                            answers  \\\n",
       "5070   5070            Никак. Любит-не любит — это вымышленные катего...   \n",
       "\n",
       "                                             ans_lemmas quest_lemmas  \\\n",
       "5070  никак любить не любить это вымышленный категор...                \n",
       "\n",
       "     ans_embeds  \n",
       "5070        NaN  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['questions']== '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ans = save_embed_corpus(df, 'answers')\n",
    "split_quest = save_embed_corpus(df, 'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_quest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14900)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anwers_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14900)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../corpora/qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../corpora/qa.csv')\n",
    "df.loc[df['questions'].isnull()] = 'нет слов'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x312 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 3120000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_bert_corpus(df['answers'], bert_model, bert_tokenizer, 'answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x312 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 3120000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_bert_corpus(df['questions'], bert_model, bert_tokenizer, 'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "ans_embeds = df['ans_lemmas'].apply(make_ft_embedding).apply(make_ft_embedding).to_numpy()\n",
    "# split_df = pd.DataFrame(df['ans_embeds'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = pd.DataFrame(df['ans_embeds'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_normalization(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = vec_normalization(np.array(emb_list))\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def save_embed_corpus(df):\n",
    "    df['ans_embeds'] = df['ans_lemmas'].apply(make_ft_embedding)\n",
    "    ans_embeds = df['ans_lemmas'].apply(make_ft_embedding).to_numpy()\n",
    "    split_df = pd.DataFrame(df['ans_embeds'].tolist(),\n",
    "                        columns=[f'word{i}' for i in range(300)])\n",
    "    split_df['doc_name'] = df['answers']\n",
    "    split_df.to_csv('../corpora/corpus_ft.csv')\n",
    "\n",
    "        \n",
    "# def save_bert_corpus(texts, model, tokenizer):\n",
    "#     vectors = []\n",
    "#     for text in texts:\n",
    "#       t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "#       with torch.no_grad():\n",
    "#         model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "#       embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "#       embeddings = torch.nn.functional.normalize(embeddings)\n",
    "#       vectors.append(embeddings[0].cpu().numpy())\n",
    "#     BERT_corpus = sparse.csr_matrix(vectors)\n",
    "#     sparse.save_npz('../corpora/BERT.npz', BERT_corpus)\n",
    "#     return sparse.csr_matrix(vectors)\n",
    "\n",
    "\n",
    "def get_query_bert(query):\n",
    "    cls_embeddings = get_bert_corpus(query, b_model, b_tokenizer)\n",
    "    return sparse.csr_matrix(cls_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save_embed_corpus(df)\n",
    "#     save_bert_corpus(df['answers'], bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "\n",
    "# Функция индексирования данных.\n",
    "# На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "def inverted_indexing(corpus, df):\n",
    "    X = tfidf_vectorizer.fit_transform(corpus)\n",
    "    data = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "    vocab = tfidf_vectorizer.get_feature_names()\n",
    "    df_ans = pd.DataFrame(data=data,columns=vocab)\n",
    "    df_ans['doc_name'] = df['answers']\n",
    "    return df_ans, X\n",
    "\n",
    "def similarity_array(query, df):\n",
    "    return cosine_similarity(query, df.values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df['ans_lemmas'].tolist()\n",
    "    corpus_df, X = inverted_indexing(corpus, df)\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/tfidf_vectorizer.pickle', 'wb'))\n",
    "    corpus_df.to_csv('../corpora/tfidf_vectorizer.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
