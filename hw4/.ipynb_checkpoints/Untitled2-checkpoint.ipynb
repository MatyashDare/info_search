{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "def vec_normalization(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = vec_normalization(np.array(emb_list))\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w.isalpha()])\n",
    "    if ans == \" \":\n",
    "        return 'пустой текст'\n",
    "    elif ans == '':\n",
    "        return 'пустой текст'\n",
    "    else:\n",
    "        return re.sub('\\n', '', ans)\n",
    "\n",
    "def make_df_from_corpus(path_to_json:str):\n",
    "    with open(path_to_json, 'r') as f:\n",
    "        qa_corpus = list(f)[:11000]\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for qa in qa_corpus:\n",
    "      qa = json.loads(qa)\n",
    "      if qa['answers'] != []:\n",
    "        max_value = -10 ** 6\n",
    "        max_text = ''\n",
    "        for answer in qa['answers']:\n",
    "          if answer['author_rating']['value'] != '':\n",
    "            cur_value = int(answer['author_rating']['value'])\n",
    "            if cur_value >= max_value:\n",
    "              max_text = answer['text']\n",
    "        if max_text != '':\n",
    "          answers.append(max_text)\n",
    "          questions.append(qa['question'])\n",
    "    df = pd.DataFrame({'questions': questions, 'answers': answers})\n",
    "    df['ans_lemmas'] = df['answers'].apply(preprocess_line)\n",
    "    return df\n",
    "\n",
    "def save_embed_corpus(df):\n",
    "    df['ans_embeds'] = df['ans_lemmas'].apply(make_ft_embedding)\n",
    "    ans_embeds = df['ans_lemmas'].apply(make_ft_embedding).to_numpy()\n",
    "    split_df = pd.DataFrame(df['ans_embeds'].tolist(),\n",
    "                        columns=[f'word{i}' for i in range(300)])\n",
    "    split_df['doc_name'] = df['answers']\n",
    "    split_df.to_csv('../corpora/corpus_ft.csv')\n",
    "\n",
    "        \n",
    "def save_bert_corpus(texts, model, tokenizer):\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "      t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "      with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "      embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "      embeddings = torch.nn.functional.normalize(embeddings)\n",
    "      vectors.append(embeddings[0].cpu().numpy())\n",
    "    BERT_corpus = sparse.csr_matrix(vectors)\n",
    "    sparse.save_npz('../corpora/BERT.npz', BERT_corpus)\n",
    "    return sparse.csr_matrix(vectors)\n",
    "\n",
    "\n",
    "def get_query_bert(query):\n",
    "    cls_embeddings = get_bert_corpus(query, b_model, b_tokenizer)\n",
    "    return sparse.csr_matrix(cls_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    save_embed_corpus(df)\n",
    "    save_bert_corpus(df['answers'], bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  гей\n",
      "Напишите число n, топ-n результатов выдачи моделей вы хотите видеть:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вот топ-10 ответов, найденных с помощью FastText: \n",
      " Ты гей?,\n",
      "Гопника,\n",
      "нигера),\n",
      "мачо,\n",
      "Шлюхи!,\n",
      "маньяк,\n",
      "Мусульманка,\n",
      "Блонда,\n",
      "социопат,\n",
      "Может он гей??! \n",
      "\n",
      "\n",
      "Вот топ-10 ответов, найденных с помощью BERT: \n",
      " фу,\n",
      "хз,\n",
      "хз,\n",
      "М,\n",
      "Ж,\n",
      "мачо,\n",
      "ум,\n",
      "лес,\n",
      "м,\n",
      "ага\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "df = pd.read_csv('../data/questions_about_love.csv')\n",
    "\n",
    "def vec_normalization(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = vec_normalization(np.array(emb_list))\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w.isalpha()])\n",
    "    if ans == \" \":\n",
    "        return 'пустой текст'\n",
    "    elif ans == '':\n",
    "        return 'пустой текст'\n",
    "    else:\n",
    "        return re.sub('\\n', '', ans)\n",
    "\n",
    "\n",
    "def cs_FastText(query, corpus_ft, cols):\n",
    "    query = preprocess_line(query)\n",
    "    scores = np.dot(corpus_ft[cols].values, make_ft_embedding(query).T)\n",
    "    argx = np.argsort(scores)[::-1]\n",
    "    return corpus_ft['doc_name'][argx.ravel()]\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix_query(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "\n",
    "def make_bert_embedding(query):\n",
    "    embeddings = embed_bert_cls(query, bert_model, bert_tokenizer)\n",
    "    return sparse.csr_matrix(embeddings)\n",
    "\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "def cs_BERT(query, sparse_matrix):\n",
    "    query = make_bert_embedding(query)\n",
    "    scores = cosine_similarity_matrix_query(sparse_matrix, query)\n",
    "    argx = np.argsort(scores, axis=0)[::-1]\n",
    "    return df['answers'][argx.ravel()]\n",
    "\n",
    "\n",
    "def main(query:str, top_n:int):\n",
    "    path_to_corpus = '../corpora/corpus_ft.csv'\n",
    "    corpus_ft = pd.read_csv(path_to_corpus)\n",
    "    cols = [col for col in corpus_ft.columns if 'word' in col]\n",
    "    tf_answers = cs_FastText(query, corpus_ft, cols)[:top_n].to_numpy()\n",
    "    sparse_matrix = sparse.load_npz('../corpora/BERT.npz')\n",
    "    bert_answers = cs_BERT(query, sparse_matrix)[:top_n].to_numpy()\n",
    "    return tf_answers, bert_answers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "    top_n = int(input('Напишите число n, топ-n результатов выдачи моделей вы хотите видеть: '))\n",
    "    tf_answers, bert_answers = main(query, top_n)\n",
    "    tf_answers = ',\\n'.join(tf_answers)\n",
    "    bert_answers = ',\\n'.join(bert_answers)\n",
    "    print(f'Вот топ-{top_n} ответов, найденных с помощью FastText: \\n', tf_answers, '\\n\\n')\n",
    "    print(f'Вот топ-{top_n} ответов, найденных с помощью BERT: \\n', bert_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_answers, bert_answers = main('не любит', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Любят.', 'нравишься', 'нравишься', 'Нравишься.', 'Нравишься'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Не любит.', 'не с любыми', 'не дружелюбие',\n",
       "       'не уважает и не любит', 'такое не бывает'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import gensim\n",
    "# from gensim.models.wrappers import FastText\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from pymystem3 import Mystem\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# russian_stopwords = stopwords.words(\"russian\")\n",
    "# mystem = Mystem()\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "# model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "# df = pd.read_csv('../data/questions_about_love.csv')\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix_query(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "\n",
    "def make_bert_embedding(query):\n",
    "    embeddings = embed_bert_cls(query, bert_model, bert_tokenizer)\n",
    "    return sparse.csr_matrix(embeddings)\n",
    "\n",
    "\n",
    "def cs_BERT(query, sparse_matrix):\n",
    "    query = make_bert_embedding(query)\n",
    "    scores = cosine_similarity_matrix_query(sparse_matrix, query)\n",
    "    argx = np.argsort(scores, axis=0)[::-1]\n",
    "    return df['answers'][argx.ravel()]\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
