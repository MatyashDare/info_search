{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = np.array(emb_list)\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w not in russian_stopwords and w.isalpha()])\n",
    "    if ans == \" \":\n",
    "        return 'пустой текст'\n",
    "    elif ans == '':\n",
    "        return 'пустой текст'\n",
    "    else:\n",
    "        return re.sub('\\n', '', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "# query = preprocess_line(query)\n",
    "# ans = cs_FastText(query)\n",
    "def cs_FastText(query):\n",
    "    path_to_corpus = '../corpora/corpus_ft.csv'\n",
    "    df = pd.read_csv(path_to_corpus)\n",
    "    cols = [col for col in df.columns if 'word' in col]\n",
    "    scores = cosine_similarity(model_ft[query].reshape((1,300)), df[cols].values)[0]\n",
    "    argx = np.argsort(scores)[::-1]\n",
    "    return df['doc_name'][argx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/questions_about_love.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "# query = preprocess_line(query)\n",
    "def cs_CountVectorizer(query):\n",
    "    count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "    sparse_matrix = sparse.load_npz('../corpora/count_vectorizer.npz')\n",
    "    query = count_vectorizer.transform([query])\n",
    "    scores = get_cosine_similarity(sparse_matrix, query)\n",
    "    argx = np.argsort(scores, axis=0)[::-1]\n",
    "    return df['answers'][argx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7034    Если человек оскорбляет - значит он не любит. ...\n",
       "3788    Какая разница любит он тебя или нет? Любовь - ...\n",
       "4688    Поскольку те, кто сохраняет девственность, обл...\n",
       "9713    У нам с молодым человеком что-то на подобие эт...\n",
       "9034    Ты правильно решила сохранить ребёнка. Из всег...\n",
       "                              ...                        \n",
       "6587                    Соответствовать его темпераменту)\n",
       "6586               Девушки созданы более эмоциональными:)\n",
       "6585          женись и тогда секс будет супружеский долг.\n",
       "6583    Уйгурки довольно симпатичные. Здесь есть одна ...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_CountVectorizer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9897                                            Не любит.\n",
       "7561                         Да за что только не любил...\n",
       "7861                                   Что он любит тебя!\n",
       "8100                                      Не любящий всех\n",
       "3413                                               Любят.\n",
       "                              ...                        \n",
       "8498                                            цкупукцап\n",
       "2941    Здравствуй . Это недоработки сервиса и правоох...\n",
       "5178                                        протеестовать\n",
       "9644                в дальневосточном федеральном округе!\n",
       "5480                                 синхронизация карбов\n",
       "Name: doc_name, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_FastText(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "# query = preprocess_line(query)\n",
    "def cs_TfidfVectorizer(query):\n",
    "    tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "    sparse_matrix = sparse.load_npz('../corpora/tfidf_vectorizer.npz')\n",
    "    query = tfidf_vectorizer.transform([query])\n",
    "    sim_array = get_cosine_similarity(sparse_matrix, query)\n",
    "    sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "    return df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7034    Если человек оскорбляет - значит он не любит. ...\n",
       "3788    Какая разница любит он тебя или нет? Любовь - ...\n",
       "4688    Поскольку те, кто сохраняет девственность, обл...\n",
       "9713    У нам с молодым человеком что-то на подобие эт...\n",
       "9034    Ты правильно решила сохранить ребёнка. Из всег...\n",
       "                              ...                        \n",
       "6587                    Соответствовать его темпераменту)\n",
       "6586               Девушки созданы более эмоциональными:)\n",
       "6585          женись и тогда секс будет супружеский долг.\n",
       "6583    Уйгурки довольно симпатичные. Здесь есть одна ...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_TfidfVectorizer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cs_BM25(query):\n",
    "    query = preprocess_line(query)\n",
    "    count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "    query_vec = count_vectorizer.transform([query])\n",
    "    sparse_matrix = sparse.load_npz('../corpora/BM25.npz')\n",
    "    scores = sparse_matrix.dot(query_vec.T).toarray()\n",
    "    sorted_scores_indx = np.argsort(scores, axis=0)[::-1]\n",
    "    return df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3413                                               Любят.\n",
       "8100                                      Не любящий всех\n",
       "7861                                   Что он любит тебя!\n",
       "800                        Если она тебя любит то конечно\n",
       "8618                               он любит себя а не вас\n",
       "                              ...                        \n",
       "6587                    Соответствовать его темпераменту)\n",
       "6586               Девушки созданы более эмоциональными:)\n",
       "6585          женись и тогда секс будет супружеский долг.\n",
       "6583    Уйгурки довольно симпатичные. Здесь есть одна ...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_BM25(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "\n",
    "\n",
    "def get_query_bert(query):\n",
    "    embeddings = embed_bert_cls(query, bert_model, bert_tokenizer)\n",
    "    return sparse.csr_matrix(embeddings)\n",
    "\n",
    "def get_cosine_similarity(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "\n",
    "def cs_BERT(query):\n",
    "#     scores = get_cosine_similarity(sparse_matrix, query)\n",
    "#     sorted_scores_indx = np.argsort(scores, axis=0)[::-1]\n",
    "    query = get_query_bert(query)\n",
    "    sparse_matrix = sparse.load_npz('../corpora/BERT.npz')\n",
    "    scores = sparse_matrix.dot(query.T).toarray()\n",
    "    sorted_scores_indx = np.argsort(scores, axis=0)[::-1]\n",
    "#     corpus = corpus[sorted_scores_indx.ravel()]\n",
    "    return df['answers'][sorted_scores_indx.ravel()]\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8185                                            танцевать\n",
       "3259                                             отвлекся\n",
       "4678                                           отношаться\n",
       "900                                                злость\n",
       "1239                                             залететь\n",
       "                              ...                        \n",
       "519     Он раскрыл твой секрет не понимая что ты обозн...\n",
       "1577    Пардон, что не по теме. В твоем сегодняшнем во...\n",
       "9966    ваша ошибка в том, что вы не объяснили парню д...\n",
       "2482    Что я могу сказать по этому поводу. С данной с...\n",
       "1753    В СССР было очень много разводов, тем не менее...\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_BERT(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_ft_embedding(query).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft[query].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer):\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    corpus_vocab = tfidf_vectorizer.get_feature_names()\n",
    "    tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
    "    cv_matrix = count_vectorizer.fit_transform(corpus)\n",
    "    idf = tfidf_vectorizer.idf_\n",
    "    idf = np.expand_dims(idf, axis=0)\n",
    "    idf = sparse.csr_matrix(idf)\n",
    "    tf = tf_vectorizer\n",
    "    pickle.dump(idf, open('../vectorizers/idf_vectorizer.pickle', 'wb'))\n",
    "    pickle.dump(tf, open('../vectorizers/tf_vectorizer.pickle', 'wb'))\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/cv_vectorizer.pickle', 'wb'))\n",
    "    return tf_matrix, idf, cv_matrix\n",
    "\n",
    "def BM_25(tf_matrix, idf_matrix, cv_matrix, k=2, b=0.75):\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    len_d = cv_matrix.sum(axis=1).T\n",
    "    avgdl = len_d.mean()\n",
    "    for i, j in zip(*tf_matrix.nonzero()):\n",
    "        A = idf_matrix[0,j] * tf_matrix[i, j] * (k+1)\n",
    "        B_1 = (k * (1 - b + b * len_d[0,i] / avgdl))\n",
    "        B_1 = np.expand_dims(B_1, axis=-1) \n",
    "        B = tf_matrix[i, j] + B_1\n",
    "        B = B[0]\n",
    "        values.append(A/B)\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "    sparse_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "    scipy.sparse.save_npz('../corpora/BM25.npz', sparse_matrix)\n",
    "#     pickle.dump(sparse_matrix, open('./BM25.pickle', 'wb'))\n",
    "    return sparse_matrix\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus('../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df.ans_lemmas.to_list()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "    tf_matrix, idf_matrix, cv_matrix = doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer)\n",
    "    BM_25(tf_matrix, idf_matrix, cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634719222.502677 1634719222.502677\n"
     ]
    }
   ],
   "source": [
    "print(time.time(), time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n",
      "Напишите число n, топ-n результатов выдачи моделей вы хотите видеть:  6\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../corpora/BERT.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f9506b99cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Введите фразу, которую хотите найти в корпусе: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Напишите число n, топ-n результатов выдачи моделей вы хотите видеть: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtf_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtf_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m',\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mbert_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m',\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1f9506b99cee>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(query, top_n)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'word'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtf_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0msparse_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpora/BERT.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mbert_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcs_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_matrix_io.py\u001b[0m in \u001b[0;36mload_npz\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mPICKLE_KWARGS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mmatrix_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'format'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../corpora/BERT.npz'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "bert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "df = pd.read_csv('../data/questions_about_love.csv')\n",
    "\n",
    "def vec_normalization(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = vec_normalization(np.array(emb_list))\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w.isalpha()])\n",
    "    if ans == \" \":\n",
    "        return 'пустой текст'\n",
    "    elif ans == '':\n",
    "        return 'пустой текст'\n",
    "    else:\n",
    "        return re.sub('\\n', '', ans)\n",
    "\n",
    "\n",
    "def cs_FastText(query, corpus_ft, cols):\n",
    "    query = preprocess_line(query)\n",
    "    scores = np.dot(corpus_ft[cols].values, make_ft_embedding(query).T)\n",
    "    argx = np.argsort(scores)[::-1]\n",
    "    return corpus_ft['doc_name'][argx.ravel()]\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix_query(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "\n",
    "def make_bert_embedding(query):\n",
    "    embeddings = embed_bert_cls(query, bert_model, bert_tokenizer)\n",
    "    return sparse.csr_matrix(embeddings)\n",
    "\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "def cs_BERT(query, sparse_matrix):\n",
    "    query = make_bert_embedding(query)\n",
    "    scores = cosine_similarity_matrix_query(sparse_matrix, query)\n",
    "    argx = np.argsort(scores, axis=0)[::-1]\n",
    "    return df['answers'][argx.ravel()]\n",
    "\n",
    "\n",
    "def main(query:str, top_n:int):\n",
    "    path_to_corpus = '../corpora/corpus_ft.csv'\n",
    "    corpus_ft = pd.read_csv(path_to_corpus)\n",
    "    cols = [col for col in corpus_ft.columns if 'word' in col]\n",
    "    tf_answers = cs_FastText(query, corpus_ft, cols)[:top_n].to_numpy()\n",
    "    sparse_matrix = sparse.load_npz('../corpora/BERT_answers.npz')\n",
    "    bert_answers = cs_BERT(query, sparse_matrix)[:top_n].to_numpy()\n",
    "    return tf_answers, bert_answers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "    top_n = int(input('Напишите число n, топ-n результатов выдачи моделей вы хотите видеть: '))\n",
    "    tf_answers, bert_answers = main(query, top_n)\n",
    "    tf_answers = ',\\n'.join(tf_answers)\n",
    "    bert_answers = ',\\n'.join(bert_answers)\n",
    "    print(f'Вот топ-{top_n} ответов, найденных с помощью FastText: \\n', tf_answers, '\\n\\n')\n",
    "    print(f'Вот топ-{top_n} ответов, найденных с помощью BERT: \\n', bert_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
