{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "model_ft = gensim.models.KeyedVectors.load('araneum_none_fasttextcbow_300_5_2018.model')\n",
    "\n",
    "def make_ft_embedding(line:str):\n",
    "    emb_list = []\n",
    "    for word in line.split():\n",
    "        emb_list .append(model_ft[word])\n",
    "    emb_list = np.array(emb_list)\n",
    "    return emb_list.mean(axis=0)\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w not in russian_stopwords and w.isalpha()])\n",
    "    if ans == \" \":\n",
    "        return 'пустой текст'\n",
    "    elif ans == '':\n",
    "        return 'пустой текст'\n",
    "    else:\n",
    "        return re.sub('\\n', '', ans)\n",
    "\n",
    "def make_df_from_corpus(path_to_json:str):\n",
    "    with open(path_to_json, 'r') as f:\n",
    "        qa_corpus = list(f)[:11000]\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for qa in qa_corpus:\n",
    "      qa = json.loads(qa)\n",
    "      if qa['answers'] != []:\n",
    "        max_value = -10 ** 6\n",
    "        max_text = ''\n",
    "        for answer in qa['answers']:\n",
    "          if answer['author_rating']['value'] != '':\n",
    "            cur_value = int(answer['author_rating']['value'])\n",
    "            if cur_value >= max_value:\n",
    "              max_text = answer['text']\n",
    "        if max_text != '':\n",
    "          answers.append(max_text)\n",
    "          questions.append(qa['question'])\n",
    "    df = pd.DataFrame({'questions': questions, 'answers': answers})\n",
    "    df['ans_lemmas'] = df['answers'].apply(preprocess_line)\n",
    "    return df\n",
    "\n",
    "def save_embed_corpus(df, text_column):\n",
    "    df['ans_embeds'] = df['ans_lemmas'].apply(make_ft_embedding)\n",
    "    split_df = pd.DataFrame(df['ans_embeds'].tolist(),\n",
    "                        columns=[f'word{i}' for i in range(300)])\n",
    "    split_df['doc_name'] = df['answers']\n",
    "    split_df.to_csv('../corpora/corpus_ft.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    save_embed_corpus(df, text_column='answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Не любит.', 'Да за что только не любил...', 'Что он любит тебя!',\n",
       "       ..., 'протеестовать', 'в дальневосточном федеральном округе!',\n",
       "       'синхронизация карбов'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model_ft = gensim.models.KeyedVectors.load('araneum_none_fasttextcbow_300_5_2018.model')\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "path_to_corpus = '../corpora/corpus_ft.csv'\n",
    "df = pd.read_csv(path_to_corpus)\n",
    "\n",
    "cols = [col for col in df.columns if 'word' in col]\n",
    "scores = cosine_similarity(model_ft[query].reshape((1,300)), df[cols].as_matrix())[0]\n",
    "argx = np.argsort(scores)[::-1]\n",
    "df['doc_name'].to_numpy()[argx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "# Функция индексирования данных.\n",
    "# На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "def inverted_indexing(corpus, df, vectorizer):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X\n",
    "\n",
    "# def similarity_array(query, df):\n",
    "#     return cosine_similarity(query, df.values)\n",
    "def get_cosine_similarity(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df['ans_lemmas'].tolist()\n",
    "    X = inverted_indexing(corpus, df, count_vectorizer)\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/count_vectorizer.pickle', 'wb'))\n",
    "    scipy.sparse.save_npz('../corpora/count_vectorizer.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7034    Если человек оскорбляет - значит он не любит. ...\n",
       "3788    Какая разница любит он тебя или нет? Любовь - ...\n",
       "4688    Поскольку те, кто сохраняет девственность, обл...\n",
       "9713    У нам с молодым человеком что-то на подобие эт...\n",
       "9034    Ты правильно решила сохранить ребёнка. Из всег...\n",
       "                              ...                        \n",
       "6587                    Соответствовать его темпераменту)\n",
       "6586               Девушки созданы более эмоциональными:)\n",
       "6585          женись и тогда секс будет супружеский долг.\n",
       "6583    Уйгурки довольно симпатичные. Здесь есть одна ...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "sparse_matrix = sparse.load_npz('../corpora/count_vectorizer.npz')\n",
    "\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "query = count_vectorizer.transform([query])\n",
    "sparse_matrix = sparse.load_npz('../corpora/count_vectorizer.npz')\n",
    "sim_array = get_cosine_similarity(sparse_matrix, query)\n",
    "sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "# Функция индексирования данных.\n",
    "# На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "def inverted_indexing(corpus, df, vectorizer):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X\n",
    "\n",
    "# def similarity_array(query, df):\n",
    "#     return cosine_similarity(query, df.values)\n",
    "def get_cosine_similarity(sparse_matrix, query):\n",
    "    return np.dot(sparse_matrix, query.T).toarray()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df['ans_lemmas'].tolist()\n",
    "    X = inverted_indexing(corpus, df, count_vectorizer)\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/tfidf_vectorizer.pickle', 'wb'))\n",
    "    scipy.sparse.save_npz('../corpora/tfidf_vectorizer.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7034    Если человек оскорбляет - значит он не любит. ...\n",
       "3788    Какая разница любит он тебя или нет? Любовь - ...\n",
       "4688    Поскольку те, кто сохраняет девственность, обл...\n",
       "9713    У нам с молодым человеком что-то на подобие эт...\n",
       "9034    Ты правильно решила сохранить ребёнка. Из всег...\n",
       "                              ...                        \n",
       "6587                    Соответствовать его темпераменту)\n",
       "6586               Девушки созданы более эмоциональными:)\n",
       "6585          женись и тогда секс будет супружеский долг.\n",
       "6583    Уйгурки довольно симпатичные. Здесь есть одна ...\n",
       "0                скажи давай встретимся, тяжело сказать ?\n",
       "Name: answers, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "sparse_matrix = sparse.load_npz('../corpora/tfidf_vectorizer.npz')\n",
    "\n",
    "\n",
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "query = tfidf_vectorizer.transform([query])\n",
    "sparse_matrix = sparse.load_npz('../corpora/tfidf_vectorizer.npz')\n",
    "sim_array = get_cosine_similarity(sparse_matrix, query)\n",
    "sorted_scores_indx = np.argsort(sim_array, axis=0)[::-1]\n",
    "df['answers'][sorted_scores_indx.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cccff7232ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_df_from_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/questions_about_love.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ef17e1151a05>\u001b[0m in \u001b[0;36mmake_df_from_corpus\u001b[0;34m(path_to_json)\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'questions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'answers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ans_lemmas'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ef17e1151a05>\u001b[0m in \u001b[0;36mpreprocess_line\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mno_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[,\\?\\!\\.\"\\:]|[a-zA-Z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# токенизируем и лемматизируем текст, приводим к нижнему регистру\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlem_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlem_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrussian_stopwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_mystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "def doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer):\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    corpus_vocab = tfidf_vectorizer.get_feature_names()\n",
    "    tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
    "    cv_matrix = count_vectorizer.fit_transform(corpus)\n",
    "    idf = tfidf_vectorizer.idf_\n",
    "    idf = np.expand_dims(idf, axis=0)\n",
    "    idf = sparse.csr_matrix(idf)\n",
    "    tf = tf_vectorizer\n",
    "    pickle.dump(idf, open('../vectorizers/idf_vectorizer.pickle', 'wb'))\n",
    "    pickle.dump(tf, open('../vectorizers/tf_vectorizer.pickle', 'wb'))\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/cv_vectorizer.pickle', 'wb'))\n",
    "    return tf_matrix, idf, cv_matrix\n",
    "\n",
    "def BM_25(tf_matrix, idf_matrix, cv_matrix, k=2, b=0.75):\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    len_d = cv_matrix.sum(axis=1).T\n",
    "    avgdl = len_d.mean()\n",
    "    for i, j in zip(*tf_matrix.nonzero()):\n",
    "        A = idf_matrix[0,j] * tf_matrix[i, j] * (k+1)\n",
    "        B_1 = (k * (1 - b + b * len_d[0,i] / avgdl))\n",
    "        B_1 = np.expand_dims(B_1, axis=-1) \n",
    "        B = tf_matrix[i, j] + B_1\n",
    "        B = B[0]\n",
    "        values.append(A/B)\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "    sparse_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "    scipy.sparse.save_npz('../corpora/BM25.npz', sparse_matrix)\n",
    "#     pickle.dump(sparse_matrix, open('./BM25.pickle', 'wb'))\n",
    "    return sparse_matrix\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus('../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df.lemmas.to_list()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "    tf_matrix, idf_matrix, cv_matrix = doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer)\n",
    "    BM_25(tf_matrix, idf_matrix, cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Функция индексирования данных.\n",
    "# На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "def inverted_indexing(corpus, df):\n",
    "    X = count_vectorizer.fit_transform(corpus)\n",
    "    data = vectorizer.fit_transform(corpus).toarray()\n",
    "    vocab = count_vectorizer.get_feature_names()\n",
    "    df_ans = pd.DataFrame(data=data,columns=vocab)\n",
    "    df_ans['doc_name'] = df['answers']\n",
    "    return df_ans, X\n",
    "\n",
    "def similarity_array(query, df):\n",
    "    return cosine_similarity(query, df.values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df['ans_lemmas'].tolist()\n",
    "    corpus_df, X = inverted_indexing(corpus, df)\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/count_vectorizer.pickle', 'wb'))\n",
    "    corpus_df.to_csv('../corpora/count_vectorizer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))\n",
    "query = count_vectorizer.transform([query])\n",
    "df = pd.read_csv('../corpora/count_vectorizer.csv')\n",
    "cols = [col for col in df.columns if col != 'doc_name']\n",
    "sim_array = similarity_array(query, df[cols])\n",
    "ans_list = df.loc[sim_array.argsort()[0][::-1]]['doc_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "\n",
    "# Функция индексирования данных.\n",
    "# На выходе создает обратный индекс, он же матрица Term-Document.\n",
    "def inverted_indexing(corpus, df):\n",
    "    X = tfidf_vectorizer.fit_transform(corpus)\n",
    "    data = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "    vocab = tfidf_vectorizer.get_feature_names()\n",
    "    df_ans = pd.DataFrame(data=data,columns=vocab)\n",
    "    df_ans['doc_name'] = df['answers']\n",
    "    return df_ans, X\n",
    "\n",
    "def similarity_array(query, df):\n",
    "    return cosine_similarity(query, df.values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df_from_corpus(path_to_json='../data/questions_about_love.jsonl')\n",
    "    df = df.head(10000)\n",
    "    corpus = df['ans_lemmas'].tolist()\n",
    "    corpus_df, X = inverted_indexing(corpus, df)\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/tfidf_vectorizer.pickle', 'wb'))\n",
    "    corpus_df.to_csv('../corpora/tfidf_vectorizer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Введите фразу, которую хотите найти в корпусе:  не любит\n"
     ]
    }
   ],
   "source": [
    "query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "query = preprocess_line(query)\n",
    "tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))\n",
    "query = tfidf_vectorizer.transform([query])\n",
    "df = pd.read_csv('../corpora/tfidf_vectorizer.csv')\n",
    "cols = [col for col in df.columns if col != 'doc_name']\n",
    "sim_array = similarity_array(query, df[cols])\n",
    "ans_list = df.loc[sim_array.argsort()[0][::-1]]['doc_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "def doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer):\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    corpus_vocab = tfidf_vectorizer.get_feature_names()\n",
    "    tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
    "    cv_matrix = count_vectorizer.fit_transform(corpus)\n",
    "    idf = tfidf_vectorizer.idf_\n",
    "    idf = np.expand_dims(idf, axis=0)\n",
    "    idf = sparse.csr_matrix(idf)\n",
    "    tf = tf_vectorizer\n",
    "    pickle.dump(idf, open('../vectorizers/idf.pickle', 'wb'))\n",
    "    pickle.dump(tf, open('../vectorizers/tf.pickle', 'wb'))\n",
    "    pickle.dump(count_vectorizer, open('../vectorizers/cv.pickle', 'wb'))\n",
    "    return tf_matrix, idf, cv_matrix\n",
    "\n",
    "def BM_25(tf_matrix, idf_matrix, cv_matrix, k=2, b=0.75):\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    len_d = cv_matrix.sum(axis=1).T\n",
    "    avgdl = len_d.mean()\n",
    "    for i, j in zip(*tf_matrix.nonzero()):\n",
    "        A = idf_matrix[0,j] * tf_matrix[i, j] * (k+1)\n",
    "        B_1 = (k * (1 - b + b * len_d[0,i] / avgdl))\n",
    "        B_1 = np.expand_dims(B_1, axis=-1) \n",
    "        B = tf_matrix[i, j] + B_1\n",
    "        B = B[0]\n",
    "        values.append(A/B)\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "    sparse_matrix = sparse.csr_matrix((values, (rows, cols)))\n",
    "    pickle.dump(sparse_matrix, open('../corpora/sparse_matrix.pickle', 'wb'))\n",
    "    return sparse_matrix\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = make_df('../data/questions_about_love.jsonl')\n",
    "    corpus = df.lemmas.to_list()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, analyzer='word', norm='l2')\n",
    "    tf_matrix, idf_matrix, cv_matrix = doc_term_matrix(corpus, tfidf_vectorizer, tf_vectorizer)\n",
    "    BM_25(tf_matrix, idf_matrix, cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "count_vectorizer = pickle.load(open('./cv.pickle', 'rb'))\n",
    "corpus_matrix = pickle.load(open('./sparse_matrix.pickle', 'rb'))\n",
    "\n",
    "\n",
    "def preprocess_line(text: str) -> str:\n",
    "    # убираем пунктуацию, оставляем только дефисы\n",
    "    no_punct = re.sub('[,\\?\\!\\.\"\\:]|[a-zA-Z]+', '', ''.join(text))\n",
    "    # токенизируем и лемматизируем текст, приводим к нижнему регистру\n",
    "    lem_words = mystem.lemmatize(text.lower())\n",
    "    ans = ' '.join([w for w in lem_words if w not in russian_stopwords and w.isalpha()])\n",
    "    return re.sub('\\n', '', ans)\n",
    "\n",
    "def count_query(query:str, corpus_matrix, doc_name):\n",
    "    query = preprocess_line(query)\n",
    "    query_vec = count_vectorizer.transform([query])\n",
    "    scores = corpus_matrix.dot(query_vec.T).toarray()\n",
    "    sorted_scores_indx = np.argsort(scores, axis=0)[::-1]\n",
    "    ans = doc_name[sorted_scores_indx.ravel()]\n",
    "    return ans\n",
    "\n",
    "def main(query:str):\n",
    "    df = pd.read_csv('./df.csv')\n",
    "    doc_name = pd.read_csv('./df.csv')['answers'].to_numpy()\n",
    "    ans = 'Найденные в порядке релевантности документы: ' + ', '.join(count_query(query, corpus_matrix, doc_name))\n",
    "    return ans\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input('Введите фразу, которую хотите найти в корпусе: ')\n",
    "    print(main(query))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
