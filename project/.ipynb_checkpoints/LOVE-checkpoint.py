import json
import gensim
from gensim.models.wrappers import FastText
from gensim.models import KeyedVectors
import numpy as np
import pandas as pd
import re
import streamlit as st
from pymystem3 import Mystem
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import scipy
from scipy import sparse
import torch
import transformers
from transformers import AutoTokenizer, AutoModel
import time
import base64


def preprocess_line(text: str) -> str:
    # —É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–µ—Ñ–∏—Å—ã
    no_punct = re.sub('[,\?\!\."\:]|[a-zA-Z]+', '', ''.join(text))
    # —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    lem_words = mystem.lemmatize(text.lower())
    ans = ' '.join([w for w in lem_words if w.isalpha()])
    if ans == " ":
        return '–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç'
    elif ans == '':
        return '–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç'
    else:
        return re.sub('\n', '', ans)

def cosine_similarity_matrix_query(sparse_matrix, query):
    return np.dot(sparse_matrix, query.T).toarray()

def vec_normalization(vec):
    return vec / np.linalg.norm(vec)

def make_ft_embedding(line:str, model_ft):
    emb_list = []
    for word in line.split():
        emb_list .append(model_ft[word])
    emb_list = vec_normalization(np.array(emb_list))
    return emb_list.mean(axis=0)

def cs_FastText(query, corpus_ft, cols, model_ft, df):
    start_time = time.time()
    scores = cosine_similarity(make_ft_embedding(query, model_ft).reshape((1,300)), corpus_ft[cols].as_matrix())[0]
    argx = np.argsort(scores)[::-1]
    end_time = time.time()
    ans_time = end_time - start_time
    return df['answers'][argx.ravel()], ans_time

def cs_CountVectorizer(query, count_vectorizer, sparse_matrix, df):
    start_time = time.time()
    query = count_vectorizer.transform([query])
    scores = cosine_similarity_matrix_query(sparse_matrix, query)
    argx = np.argsort(scores, axis=0)[::-1]
    end_time = time.time()
    ans_time = end_time - start_time
    return df['answers'][argx.ravel()], ans_time


def cs_TfidfVectorizer(query, tfidf_vectorizer, sparse_matrix, df):
    start_time = time.time()
    query = tfidf_vectorizer.transform([query])
    scores = cosine_similarity_matrix_query(sparse_matrix, query)
    argx = np.argsort(scores, axis=0)[::-1]
    end_time = time.time()
    ans_time = end_time - start_time
    return df['answers'][argx.ravel()], ans_time


def cs_BM25(query, count_vectorizer, sparse_matrix, df):
    start_time = time.time()
    query = count_vectorizer.transform([query])
    scores = cosine_similarity_matrix_query(sparse_matrix, query)
    argx = np.argsort(scores, axis=0)[::-1]
    end_time = time.time()
    ans_time = end_time - start_time
    return df['answers'][argx.ravel()], ans_time

def make_bert_embedding(query, bert_model, bert_tokenizer):
    embeddings = embed_bert_cls(query, bert_model, bert_tokenizer)
    return sparse.csr_matrix(embeddings)


def cs_BERT(query, sparse_matrix, bert_model, bert_tokenizer, df):
    start_time = time.time()
    query = make_bert_embedding(query, bert_model, bert_tokenizer)
    scores = cosine_similarity_matrix_query(sparse_matrix, query)
    argx = np.argsort(scores, axis=0)[::-1]
    end_time = time.time()
    ans_time = end_time - start_time
    return df['answers'][argx.ravel()], ans_time

def embed_bert_cls(text, model, tokenizer):
    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        model_output = model(**{k: v.to(model.device) for k, v in t.items()})
    embeddings = model_output.last_hidden_state[:, 0, :]
    embeddings = torch.nn.functional.normalize(embeddings)
    return embeddings[0].numpy()




@st.cache(allow_output_mutation=True)
def data_loader():
    d = {}
    bert_tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny")
    bert_model = AutoModel.from_pretrained("cointegrated/rubert-tiny")
    model_ft = gensim.models.KeyedVectors.load('../vectorizers/araneum_none_fasttextcbow_300_5_2018.model')
    df = pd.read_csv('../corpora/qa.csv')
    d['bert_tokenizer'] = bert_tokenizer
    d['bert_model'] = bert_model
    d['model_ft'] = model_ft
    d['df'] = df
    d['mystem'] = Mystem()
    
    count_vectorizer = pickle.load(open('../vectorizers/count_vectorizer.pickle', 'rb'))
    sparse_matrix_count_vectorizer = sparse.load_npz('../corpora/count_vectorizer_questions.npz')
    d['count_vectorizer'] = count_vectorizer
    d['sparse_matrix_count_vectorizer'] = sparse_matrix_count_vectorizer
    sparse_matrix_BM25 = sparse.load_npz('../corpora/BM25_questions.npz')
    d['sparse_matrix_BM25'] = sparse_matrix_BM25
    
    tfidf_vectorizer = pickle.load(open('../vectorizers/tfidf_vectorizer.pickle', 'rb'))
    sparse_matrix_tfidf_vectorizer = sparse.load_npz('../corpora/tfidf_vectorizer_questions.npz')
    d['tfidf_vectorizer'] = tfidf_vectorizer
    d['sparse_matrix_tfidf_vectorizer'] = sparse_matrix_tfidf_vectorizer
    
    corpus_ft = pd.read_csv('../corpora/corpus_ft_questions.csv')
    cols = [col for col in corpus_ft.columns if 'word' in col]
    d['corpus_ft'] = corpus_ft
    d['cols'] = cols
    
    sparse_matrix_BERT = sparse.load_npz('../corpora/BERT_questions.npz')
    d['sparse_matrix_BERT'] = sparse_matrix_BERT
    return d


    
def main():   
    st.title('ü•∞')
    st.title('LOVE SEARCH')
    st.header('–ü—Ä–æ–±–ª–µ–º–∞ —Å –ª–∏—á–Ω–æ–π –∂–∏–∑–Ω—å—é? –ú–æ–∂–µ—Ç, –∑–¥–µ—Å—å –ø–æ–ª—É—á–∏—Ç—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç, –±–ª–∏–∑–∫–∏–π —Å–µ—Ä–¥—Ü—É?')
    st.subheader('–ö–æ—Ä–ø—É—Å —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞ –æ—Ç–≤–µ—Ç–∞—Ö –ú–µ–π–ª.—Ä—É. ¬†–û–Ω —Å–º–µ—à–Ω–æ–π, –Ω–µ –±–µ—Ä–∏—Ç–µ –æ—Ç–≤–µ—Ç—ã –±–ª–∏–∑–∫–æ –∫ —Å–µ—Ä–¥—Ü—É, –µ—Å–ª–∏ –æ–Ω–∏ –≤–∞–º –≤–¥—Ä—É–≥ –Ω–µ –ø–æ–Ω—Ä–∞–≤—è—Ç—Å—è')
    st.subheader('–ú—ã –∏—â–µ–º –≤ –∫–æ—Ä–ø—É—Å–µ –≤–æ–ø—Ä–æ—Å—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –≤–∞—à–µ–º—É, –∏ –≤—ã–¥–∞–µ–º –æ—Ç–≤–µ—Ç—ã –Ω–∞ –Ω–∏—Ö')
    st.write('–ù–∏–∂–µ –º–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –≤–∞—à –∑–∞–ø—Ä–æ—Å –∏ –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –ø–æ—Å–ª—É—à–∞—Ç—å –ø—Ä–µ–∫—Ä–∞—Å–Ω—É—é –º—É–∑—ã–∫—É')
    audio_file = open('audio.wav', 'rb')
    audio_bytes = audio_file.read()
    st.audio(audio_bytes, format='audio/wav')
    query = st.text_area('–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å')
    choice = st.selectbox('–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø–æ–∏—Å–∫',
                          ['CountVectorizer', 'TfidfVectorizer', 'BM25', 'FastText', 'BERT'])
    top_n = st.slider('–í—ã–±–µ—Ä–∏—Ç–µ —á–∏—Å–ª–æ —Ç–æ–ø-–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞', 1, 50, 5)
    if st.button('–∏—Å–∫–∞—Ç—å'):
        if query == '':
            st.error('–ù–µ –Ω—É–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å –ø—É—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã!')
        else:
            ans_time = -1000
            ans, ans_time = query_return(query, choice, top_n)
            ans_df = pd.DataFrame({'–æ—Ç–≤–µ—Ç—ã': ans})
            st.write('–í–æ—Ç, —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ (P.S.: –º–æ–∂–Ω–æ —Ç–∞–±–ª–∏—Ü—É —É–≤–µ–ª–∏—á–∏—Ç—å –≤ —Ä–∞–∑–º–µ—Ä–µ –∏–ª–∏ –∂–µ –Ω–∞–≤–µ—Å—Ç–∏ –º—ã—à–∫–æ–π –Ω–∞ –ø–æ–Ω—Ä–∞–≤–∏–≤—à–∏–π—Å—è –æ—Ç–≤–µ—Ç –∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –µ—à–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é):')
            st.dataframe(ans_df)
            st.write(f"–í—Å–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞—à–ª–∏—Å—å –∑–∞ {ans_time} —Å–µ–∫—É–Ω–¥!" )
            st.success('–ü–æ–∏—Å–∫ –ø—Ä–æ—à–µ–ª –±–µ–∑ –æ—à–∏–±–æ–∫!')
            if ans_time != -1000:
                st.balloons()
    st.write('–ï—Å–ª–∏ –≤–∞–º –Ω–µ –Ω—Ä–∞–≤—è—Ç—Å—è –æ—Ç–≤–µ—Ç—ã, –º–æ–∂–Ω–æ –Ω–∞–∂–∞—Ç—å –Ω–∞ –∫–Ω–æ–ø–∫—É "–í–∏–¥–µ–æ" –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–º–µ—à–Ω–æ–µ –≤–∏–¥–µ–æ')
    if st.button('–í–∏–¥–µ–æ'):
        video_url  = 'https://www.youtube.com/watch?v=OGnDljE4R2g'
        st.video(video_url)

def query_return(query, choice, top_n):
    if choice == 'CountVectorizer':
        count_vectorizer = d['count_vectorizer']
        sparse_matrix = d['sparse_matrix_count_vectorizer']
        ans_ranged, time = cs_CountVectorizer(query, count_vectorizer, sparse_matrix, df)
    elif choice == "TfidfVectorizer":
        tfidf_vectorizer = d['tfidf_vectorizer']
        sparse_matrix = d['sparse_matrix_tfidf_vectorizer']
        ans_ranged, time = cs_TfidfVectorizer(query, tfidf_vectorizer, sparse_matrix, df)
    elif choice == "BM25":
        count_vectorizer = d['count_vectorizer']
        sparse_matrix = d['sparse_matrix_BM25']
        ans_ranged, time = cs_BM25(query, count_vectorizer, sparse_matrix, df)
    elif choice == "FastText":
        corpus_ft = d['corpus_ft']
        cols = d['cols']
        model_ft = d['model_ft']
        ans_ranged, time = cs_FastText(query, corpus_ft, cols, model_ft, df)
    elif choice == "BERT":
        sparse_matrix = d['sparse_matrix_BERT']
        bert_model = d['bert_model']
        bert_tokenizer = d['bert_tokenizer']
        ans_ranged, time = cs_BERT(query, sparse_matrix, bert_model, bert_tokenizer, df)
    return ans_ranged[:top_n].tolist(), time




def get_base64(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()


def set_background(png_file):
    bin_str = get_base64(png_file)
    page_bg_img = '''
    <style>
    .stApp {
      background-image: url("data:image/png;base64,%s");
      background-size: cover;
    }
    </style>
    ''' % bin_str
    st.markdown(page_bg_img, unsafe_allow_html=True)


set_background('background.jpg')


d = data_loader()
df = d['df']
main()